---
title: Captioning with speech to text - Speech service
titleSuffix: Azure Cognitive Services
description: An overview of key concepts for captioning with speech to text.
services: cognitive-services
author: eric-urban
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: conceptual
ms.date: 03/10/2022
ms.author: eur
---

# Captioning with speech to text

Use captioning with speech to text to transcribe the spoken words into text. Captioning can accompany real time or pre-recorded speech. 

Here are some common captioning scenarios:
- Online courses and instructional videos
- Sporting events
- Voice and video calls

This guide covers captioning for speech, but does not include speaker ID or sound effects.

The following are aspects to consider when using captioning:
* Center captions horizontally on the screen, in a large and prominent font. 
* Let your audience know that captions are generated by an automated service.
* Consider how many words can fit on the screen at a time. 
* Learn about captioning protocols such as [SMPTE-TT](https://ieeexplore.ieee.org/document/7291854). 
* Consider output formats such as SRT (SubRip Subtitle) and WebVTT (Web Video Text Tracks).

> [!TIP]
> Try the [Azure Video Analyzer for Media](/azure/azure-video-analyzer/video-analyzer-for-media-docs/video-indexer-overview) as a demonstration of how you can get captions for videos that you upload. 

Whether you are showing captions in real time or with a recording, you can use the [Speech SDK](speech-sdk.md) to recognize speech and get transcriptions. You can also use the [Batch transcription API](batch-transcription.md) for pre-recorded video. This guide shows examples using the speech SDK.

## Caption and speech synchronization 

You'll want to synchronize captions with the audio track, whether it's done in real time or with a prerecording. 

The Speech service returns the offset and duration of the recognized speech. 

- **Offset**: Used to measure the relative position of the speech that is currently being recognized, from the time that you started speech recognition. Speech recognition does not necessarily start at the beginning of the audio track. Offset is measured in ticks, where a single tick represents one hundred nanoseconds or one ten-millionth of a second.
- **Duration**: Duration of the utterance that is being recognized. The duration time span does not include trailing or leading silence. 

As soon as you start continuous recognition, the offset starts incrementing in ticks from `0` (zero). 

```csharp
// Starts continuous recognition. Use StopContinuousRecognitionAsync() to stop recognition.
await speechRecognizer.StartContinuousRecognitionAsync().ConfigureAwait(false);
```

The end of a single utterance is determined by listening for silence at the end. You won't get the final recognition result until an utterance has completed. Recognizing events will provide intermediate results that are subject to change while an audio stream is being processed. Recognized events will provide the final transcribed text once processing of an utterance is completed.

> [!NOTE]
> Punctuation of intermediate results is not available. 

With the `Recognizing` event, you can get the offset and duration of the speech being recognized. Offset and duration per word are not available while recognition is in progress. Each `Recognizing` event comes with a textual estimate of the speech recognized so far.

This code snippet shows how to get the offset and duration from a `Recognizing` event. 

```csharp
speechRecognizer.Recognizing += (object sender, SpeechRecognitionEventArgs e) =>
    {
        if (e.Result.Reason == ResultReason.RecognizingSpeech)
        {        
            Console.WriteLine($"RECOGNIZING: Text={e.Result.Text}");
            Console.WriteLine(String.Format ("Offset in Ticks: {0}", e.Result.OffsetInTicks));
            Console.WriteLine(String.Format ("Duration in Ticks: {0}", e.Result.Duration.Ticks));
        }
    };
```

Once an utterance has been recognized, you can get the offset and duration of the recognized speech. With the `Recognized` event, you can also get the offset and duration per word. To request the offset and duration per word, first you must set the corresponding `SpeechConfig` property as shown here:

```csharp
speechConfig.RequestWordLevelTimestamps();
```

This code snippet shows how to get the offset and duration from a `Recognized` event. 

```csharp
speechRecognizer.Recognized += (object sender, SpeechRecognitionEventArgs e) =>
    {
        if (ResultReason.RecognizedSpeech == e.Result.Reason && e.Result.Text.Length > 0)
        {            
            Console.WriteLine($"RECOGNIZED: Text={e.Result.Text}");
            Console.WriteLine(String.Format ("Offset in Ticks: {0}", e.Result.OffsetInTicks));
            Console.WriteLine(String.Format ("Duration in Ticks: {0}", e.Result.Duration.Ticks));
                        
            var detailedResults = speechRecognitionResult.Best();
            if(detailedResults != null && detailedResults.Any())
            {
                // The first item in detailedResults corresponds to the recognized text.
                // This is not necessarily the item with the highest confidence number.
                var bestResults = detailedResults?.ToList()[0];
                Console.WriteLine($"\tConfidence: {bestResults.Confidence}\n\tText: {bestResults.Text}\n\tLexicalForm: {bestResults.LexicalForm}\n\tNormalizedForm: {bestResults.NormalizedForm}\n\tMaskedNormalizedForm: {bestResults.MaskedNormalizedForm}");
                
                // You must set speechConfig.RequestWordLevelTimestamps() to get word-level timestamps.
                Console.WriteLine($"\tWord-level timing:");
                Console.WriteLine($"\t\tWord | Offset | Duration");
                Console.WriteLine($"\t\t----- | ----- | ----- ");

                foreach (var word in bestResults.Words)
                {
                    Console.WriteLine($"\t\t{word.Word} | {word.Offset} | {word.Duration}");
                }
            }
        }
    };
```

The following table shows potential offset and duration in ticks when a speaker says "Welcome to Applied Mathematics course 201." For each utterance, the offset doesn't change throughout the `Recognized` and `Recognized` events. The duration of speech recognized so far is calculated as an offset from the beginning of the utterance.

|Event  |Text  |Offset (in ticks)  |Duration (in ticks) |
|---------|---------|---------|---------|
|RECOGNIZING     |welcome         |17000000         |5000000         |
|RECOGNIZING     |welcome to         |17000000         |6400000         |
|RECOGNIZING     |welcome to applied math          |17000000         |13600000         |
|RECOGNIZING     |welcome to applied mathematics         |17000000         |17200000         |
|RECOGNIZING     |welcome to applied mathematics course         |17000000         |23700000         |
|RECOGNIZING     |welcome to applied mathematics course 2         |17000000         |26700000         |
|RECOGNIZING     |welcome to applied mathematics course 201         |17000000         |33400000         |
|RECOGNIZED     |Welcome to applied Mathematics course 201.         |17000000         |34500000         |

The total duration of the first utterance was 3.45 seconds. It was recognized at 1.7 to 5.15 seconds offset from the start of speech recognition (00:00:01.700 --> 00:00:05.150).

If the speaker continues to say "Let's get started," a new offset is calculated. Again, the offset is always calculated from the start of recognition to the start of an utterance. The following table shows potential offset and duration for an utterance that was recognized two seconds after the previous utterance ended.

|Event  |Text  |Offset (in ticks)  |Duration (in ticks) |
|---------|---------|---------|---------|
|RECOGNIZING     |OK         |71500000         |3100000         |
|RECOGNIZING     |OK now         |71500000         |10300000         |
|RECOGNIZING     |OK, now let's         |71500000         |14700000         |
|RECOGNIZING     |OK, now let's get started.         |71500000         |18500000         |
|RECOGNIZED     |OK, now let's get started.         |71500000         |20600000         |

The total duration of the second utterance was 2.06 seconds. It was recognized at 7.15 to 9.21 seconds offset from the start of speech recognition (00:00:07.150 --> 00:00:09.210). 

## Stable partial intermediate results

Consider when to start displaying captions. Speech recognition results are subject to change while an utterance is still being recognized. Partial intermediate results are returned with each `Recognizing` event. As each word is processed, the Speech service re-evaluates an utterance in the new context and again returns the best result. The new result is not guaranteed to be the same as the previous result. The complete and final transcription of an utterance is returned with the `Recognized` event.

For captioning of prerecorded speech or wherever latency is not a concern, you could wait for the complete transcription of each utterance before displaying any words. Given the final offset and duration of each word in an utterance, you know when to show subsequent words at pace with the soundtrack.

Real time captioning presents additional challenges with respect latency versus accuracy. Do you show the text from each `Recognizing` event, even if the results could change? 

In the following recognition sequence, "math" was recognized as a word, but the final text was "mathematics". At another point, "course 2" was recognized, but the final text was "course 201". 

```console
RECOGNIZING: Text=welcome to
RECOGNIZING: Text=welcome to applied math
RECOGNIZING: Text=welcome to applied mathematics
RECOGNIZING: Text=welcome to applied mathematics course 2
RECOGNIZING: Text=welcome to applied mathematics course 201
RECOGNIZED: Text=Welcome to applied Mathematics course 201.
```

In the previous example, the transcriptions were additive and no text was retracted. But at other times you might find that the intermediate results were inaccurate. In any case, the unstable intermediate results can be perceived as "flickering" when displayed. 

You can request that the Speech service send you more stable results. You would effectively be asking the Speech service to make sure the words are recognized multiple times before returning intermediate `Recognizing` events. This is done by setting the `SpeechServiceResponse_StablePartialResultThreshold` property to a value between `0` and `2147483647`. 

The value that you set is the number of times a word has to be in partial results to be returned. For example, if you set the `SpeechServiceResponse_StablePartialResultThreshold` value to `5`, the Speech service will affirm recognition of a word at least five times before returning the partial results to you with a `Recognizing` event.

```csharp
speech_config.SetProperty (PropertyId.SpeechServiceResponse_StablePartialResultThreshold, 5);
```

If the stable partial result threshold is set to `5`, for this example, no words are altered or backtracked.

```console
RECOGNIZING: Text=welcome to
RECOGNIZING: Text=welcome to applied
RECOGNIZING: Text=welcome to applied mathematics
RECOGNIZED: Text=Welcome to applied Mathematics course 201.
```

In summary, requesting more stable partial results will reduce the "flickering," but can increase latency as you wait for higher confidence results. 

## Profanity filter 

You can specify whether to mask, remove, or show profanity in recognition results. 

> [!NOTE]
> Microsoft also reserves the right to mask or remove any word that is deemed inappropriate. Such words will not be returned by the Speech service, whether or not you enabled profanity filtering.

The profanity filter options are:
- `Masked`: Replaces letters in profane words with asterisk (*) characters.
- `Raw`: Include the profane words verbatim.
- `Removed`: Removes profane words.

For example, to remove profane words from the speech recognition result, set the profanity filter to `Removed` as shown here:

```csharp
speech_config.SetProfanity(ProfanityOption.Removed);
```

Profanity filter is applied:
SpeechRecognitionResult.Text
DetailedSpeechRecognitionResult.Text
DetailedSpeechRecognitionResult.MaskedNormalizedForm

Profanity filter is not applied:
DetailedSpeechRecognitionResult.LexicalForm
DetailedSpeechRecognitionResult.NormalizedForm
DetailedSpeechRecognitionResult.Words


## Capitalize intermediate results

You can choose to capitalize the partial intermediate results. This is useful if you want to display the partial results in a more natural way. 

To request capitalization of partial intermediate results, set the `SpeechServiceResponse_PostProcessingOptions` property to `TrueText` as shown here:

```csharp
speech_config.SetProperty ("SpeechServiceResponse_PostProcessingOption", "TrueText");
```

## Improve recognition accuracy

A [phrase list](improve-accuracy-phrase-list.md) is a list of words or phrases that you provide right before starting speech recognition. Adding a phrase to a phrase list increases its importance, thus making it more likely to be recognized. 

Examples of phrases include:
* Names
* Geographical locations
* Homonyms
* Words or acronyms unique to your industry or organization

There are some situations where [training a custom model](custom-speech-overview.md) is likely the best option to improve accuracy. For example, if you are captioning orthodontics lectures, you might want to train a custom model with the corresponding domain data.

## Language identification

If the language in the audio could change, use continuous [language identification](language-identification.md). Language identification is used to identify languages spoken in audio when compared against a list of [supported languages](language-support.md#language-identification). You provide up to 10 candidate languages, at least one of which is expected be in the audio. The Speech service returns the most likely language in the audio. 

## Next steps
* [Get started with speech to text](get-started-speech-to-text.md)
