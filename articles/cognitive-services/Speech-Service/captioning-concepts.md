---
title: Captioning with speech to text - Speech service
titleSuffix: Azure Cognitive Services
description: An overview of key concepts for captioning with speech to text.
services: cognitive-services
author: eric-urban
manager: nitinme
ms.service: cognitive-services
ms.subservice: speech-service
ms.topic: conceptual
ms.date: 03/10/2022
ms.author: eur
---

# Captioning with speech to text

Use captioning with speech to text to transcribe the spoken words into text. Captioning can accompany real time or pre-recorded speech. 

Here are some common captioning scenarios:
- Online courses and instructional videos
- Sporting events
- Voice and video calls

This guide covers captioning for speech, but does not include speaker ID or sound effects.

The following are additional aspects to consider:
* Center captions horizontally on the screen, in a large and prominent font. 
* Let your audience know that captions are generated by an automated service.
* Consider how many words and lines can fit on the screen. 
* Learn about captioning protocols such as [SMPTE-TT](https://ieeexplore.ieee.org/document/7291854). 
* Consider output formats such as SRT (SubRip Subtitle) and WebVTT (Web Video Text Tracks).

> [!TIP]
> Try the [Azure Video Analyzer for Media](/azure/azure-video-analyzer/video-analyzer-for-media-docs/video-indexer-overview) as a demonstration of how you can get captions for videos that you upload. 


## Real time versus offline captioning

You could also use the [Batch transcription API](batch-transcription.md) to transcribe captions for a pre-recorded video. 

## Caption and speech synchronization 

The Speech service returns the offset and duration of the recognized speech. You can use this information to help synchronize the captions with the audio track, whether it's streamed in real time or a prerecorded playback. 

Captions should be synchronized 
to the speed of the person talking.

- `OffsetInTicks`: Offset of the recognized speech in ticks. A single tick represents one hundred nanoseconds or one ten-millionth of a second.
- `Duration`: Duration of the recognized speech as a time span. The duration does not include trailing or leading silence.

```csharp
var startDateTime = new DateTime (e.Result.OffsetInTicks);
var endDateTime = startDateTime.Add (e.Result.Duration);
```

If you want detailed recognition results with word-level offset and duration, set the `SpeechConfig` property as shown here:
```csharp
speechConfig.RequestWordLevelTimestamps();
```

for the `SpeechRecognizer` has a As soon as continuous recognition is started, the duration offset begins incrementing in ticks from `0` (zero). 

```csharp
// Starts continuous recognition. Use StopContinuousRecognitionAsync() to stop recognition.
await speech_recognizer.StartContinuousRecognitionAsync().ConfigureAwait(false);
```

While recognizing, you can get the offset and current duration of the speech. Details per word are not available while recognition is in progress.

```csharp
speech_recognizer.Recognizing += (object sender, SpeechRecognitionEventArgs e) =>
    {
        if (e.Result.Reason == ResultReason.RecognizingSpeech)
        {        
            Console.WriteLine($"RECOGNIZING: Text={e.Result.Text}");
            Console.WriteLine(String.Format ("Offset in Ticks: {0}", e.Result.OffsetInTicks));
            Console.WriteLine(String.Format ("Duration in Ticks: {0}", e.Result.Duration.Ticks));
        }
    };
```

Once an utterance has been recognized, you can get the offset and final duration of the recognized speech. With the recognized event, you can also get the offset and duration per word. 

```csharp
speech_recognizer.Recognized += (object sender, SpeechRecognitionEventArgs e) =>
    {
        if (ResultReason.RecognizedSpeech == e.Result.Reason && e.Result.Text.Length > 0)
        {            
            Console.WriteLine($"RECOGNIZED: Text={e.Result.Text}");
            Console.WriteLine(String.Format ("Offset in Ticks: {0}", e.Result.OffsetInTicks));
            Console.WriteLine(String.Format ("Duration in Ticks: {0}", e.Result.Duration.Ticks));
                        
            var detailedResults = speechRecognitionResult.Best();
            if(detailedResults != null && detailedResults.Any())
            {
                // The first item in detailedResults corresponds to the recognized text.
                // This is not necessarily the item with the highest confidence number.
                var bestResults = detailedResults?.ToList()[0];
                Console.WriteLine($"\tConfidence: {bestResults.Confidence}\n\tText: {bestResults.Text}\n\tLexicalForm: {bestResults.LexicalForm}\n\tNormalizedForm: {bestResults.NormalizedForm}\n\tMaskedNormalizedForm: {bestResults.MaskedNormalizedForm}");
                
                // You must set speechConfig.RequestWordLevelTimestamps() to get word-level timestamps.
                Console.WriteLine($"\tWord-level timing:");
                Console.WriteLine($"\t\tWord | Offset | Duration");
                Console.WriteLine($"\t\t----- | ----- | ----- ");

                foreach (var word in bestResults.Words)
                {
                    Console.WriteLine($"\t\t{word.Word} | {word.Offset} | {word.Duration}");
                }
            }
        }
    };
```


## Stable partial intermediate results

For captioning of prerecorded speech, typically you can wait for the final recognized transcription.  Then You will  before synchronizing it with the speech . 

Real time captioning presents unique challenges with respect latency versus accuracy. Events in real time are captured by the microphone and are processed by the speech recognition service.

The end of a single utterance is determined by listening for silence at the end or until a maximum of 15 seconds of audio is processed. The task returns the recognized text

You won't get the final recognition result until an utterance has completed. Recognizing events will provide the intermediate recognized text while an audio stream is being processed. Recognized events will provide the final recognized text once audio capture is completed

Recognizing events contain intermediate recognition results.
Recognized events contain final recognition results, which indicate a successful recognition attempt.

For example, if a speaker says "Welcome to applied Mathematics course 201."

```console
RECOGNIZING: Text=welcome to
RECOGNIZING: Text=welcome to applied math
RECOGNIZING: Text=welcome to applied mathematics
RECOGNIZING: Text=welcome to applied mathematics course 2
RECOGNIZING: Text=welcome to applied mathematics course 201
RECOGNIZED: Text=Welcome to applied Mathematics course 201.
```

Consider how often you want to show captions, and whether you can wait for the final recognized text. To show captions as soon as possible, even if recognition is not yet completed, you can use intermediate text with the `Recognizing` event. If you want to show captions only when recognition is complete, wait for the final text with the `Recognized` event.

Stable partials will lead to less "flickering" but perhaps have more delay in showing the correct result. Supporting punctuation on intermediate results is not supported. 

Setting the stable partial result threshold to 0 will return all partial results.


```csharp
// The number of times a word has to be in partial results to be returned. 
speech_config.SetProperty (PropertyId.SpeechServiceResponse_StablePartialResultThreshold, 5);
```


Setting stable partial result threshold to 2:

```console
RECOGNIZING: Text=welcome
RECOGNIZING: Text=welcome to
RECOGNIZING: Text=welcome to applied
RECOGNIZING: Text=welcome to applied mathematics
RECOGNIZING: Text=welcome to applied mathematics course
RECOGNIZING: Text=welcome to applied mathematics course 2
RECOGNIZING: Text=welcome to applied mathematics course 20
RECOGNIZED: Text=Welcome to applied Mathematics course 201.
```

Setting stable partial result threshold to 5:

```console
RECOGNIZING: Text=welcome to
RECOGNIZING: Text=welcome to applied
RECOGNIZING: Text=welcome to applied mathematics
RECOGNIZED: Text=Welcome to applied Mathematics course 201.
```


## Profanity filter 

Removes profanity (swearing), or replaces letters of profane words with stars
Masked: Replaces letters in profane words with star characters.
Removed: Removes profane words.

This enables profanity filter:
```csharp
speech_config.SetProfanity (ProfanityOption.Removed);
```

## Capitalize intermediate results
For more readable result, capitalization can help. 

Capitalizes intermediates? But does not add punctuation? 
Semi-display processing?
Lexical form is just the letters in a transcript (e.g., six - not '6'), no caps, etc

```csharp
// A string value specifying which post processing option should be used by service. Allowed value: TrueText
speech_config.SetProperty ("SpeechServiceResponse_PostProcessingOption", "TrueText");
```

## Improve recognition accuracy

For specific terms/names (e.g. captioning a sports game), Phrase list would help.

For general topics (e.g. captioning of orthodontist lectures), customization would help. Hereâ€™s how you can create a custom model



## Language identification
Multi-lingual
Short switches not supported well (in the LID doc already)



## Next steps
* [Get started with speech to text](get-started-speech-to-text.md)
