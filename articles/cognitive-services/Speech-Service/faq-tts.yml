### YamlMime:FAQ
metadata:
  title: Text-to-speech FAQ
  titleSuffix: Azure Cognitive Services
  description: Get answers to frequently asked questions about the text-to-speech service.
  services: cognitive-services
  author: eric-urban
  manager: nitinme
  ms.service: cognitive-services
  ms.subservice: speech-service
  ms.topic: faq
  ms.date: 03/27/2023
  ms.author: eur
title: Text-to-speech FAQ
summary: |
  This article answers commonly asked questions about the text-to-speech service. If you can't find answers to your questions here, check out [other support options](../cognitive-services-support-options.md?context=%2fazure%2fcognitive-services%2fspeech-service%2fcontext%2fcontext%253fcontext%253d%2fazure%2fcognitive-services%2fspeech-service%2fcontext%2fcontext).
  

sections:
  - name: General
    questions:
      - question: |
          How would we disclose to the end user that the voice used in the game is a synthetic voice?
        answer: |
          There are several ways to disclose the synthetic nature of the voice including implicit and explicit byline. Refer to [Disclosure design guidelines](/legal/cognitive-services/speech-service/custom-neural-voice/concepts-disclosure-guidelines?context=/azure/cognitive-services/speech-service/context/context).
      - question: |
          What audio formats does Text-to-Speech support?
        answer: |
          The Speech service supports 48-kHz, 24-kHz, 16-kHz, and 8-kHz audio outputs. Each prebuilt neural voice model is available at 24kHz and high-fidelity 48kHz. See [Audio outputs](rest-text-to-speech.md?tabs=streaming#audio-outputs).
      - question: |
          How can we balance dynamic and static content to limit cost?
        answer: |
          You can cache the real-time generated content, and fall back to cloud with best quality, as we are improving the model quality over time. If you do not have strong demand on latency, you can use our real-time API to generate all content, cache it and serve it on an as-needed basis.
      - question: |
          Can we make the dialogue sound even more natural by inserting er, um, stutter, pause, or repeated words and so on?
        answer: |
          We are evaluating spontaneous speech synthesis to automatically insert the filled pause (such as um, uh) and synthesize the speech accordingly. But this isn't on the current roadmap.
      - question: |
          Is there a mapping between Viseme IDs and mouth shape?
        answer: |
          Yes. See [Get facial position with viseme](how-to-speech-synthesis-viseme.md?tabs=visemeid#map-phonemes-to-visemes).
      - question: |
          Can the Visemes be mapped to UE5 MetaHuman blend shapes weights?
        answer: |
          UE5 MetaHuman is now using a newly defined driven parameter named Expression, however, we could still use blend shapes to drive MetaHuman. One of our customers has done this and successfully driven the prefix UE5 Avatar.

  - name: Speech synthesis markup language (SSML)
    questions:
      - question: |
          Can the voice be customized to stress specific words?
        answer: |
          Adjusting the emphasis is supported for some voices depending on the locale. See the [emphasis tag](speech-synthesis-markup-voice.md#adjust-emphasis).
      - question: |
          Can we have multiple strength for each emotions, like very sad, slightly sad and so on in?
        answer: |
          Adjusting the style degree is supported for some voices depending on the locale. See the [mstts:express-as tag](speech-synthesis-markup-voice.md#speaking-styles-and-roles).

  - name: Custom Neural Voice
    questions:
      - question: |
          How much data is required to create a custom neural voice?
        answer: |
          At least 300 lines of recordings (or approximately 30 minutes of speech) are required as training data for Custom Neural Voice. We recommend 2,000 lines of recordings (or approximately 2-3 hours of speech) to create a voice for production use. The more data you record the better the quality of the voice. For the script selection criteria, see [Record custom voice samples](record-custom-voice-samples.md).
      - question: |
          Can we create multi-styles for the same custom neural voice and host multi-styles in one model to save our cost?
        answer: |
          The [cross lingual](how-to-custom-voice-create-voice.md?tabs=crosslingual#train-your-custom-neural-voice-model) feature supports multi-style for the same voice in one model.
      - question: |
          What about languages that have different pronunciation structure and assembly? For example, English and Japanese don't form a sentence in the same way. If a voice talent has a fast to slow cadence on a phrase in English, would that map correctly across the same phrase in Japanese?
        answer: |
          Each neural voice is trained with audio data recorded by native speaking voice talent. For cross-lingual voice, we transfer the major features like timbre to sound like the original speaker and preserve the right pronunciation. It will use the native way to speak Japanese and still sound similar (but not exactly) like the original English speaker. 
      - question: |
          Can we include duplicate text sentences in the same set of training data?
        answer: |
          No. The service will flag the duplicate sentences and just keep the first imported one. For the script selection criteria, see [Record custom voice samples](record-custom-voice-samples.md).
      - question: |
          Can we include multiple styles in the same set of training data?
        answer: |
          We recommend that you keep the style consistent in one set of training data. If the styles are different, put them into different training sets. In this case, you may consider using the multi-style voice training feature of Custom Neural Voice. For the script selection criteria, see [Record custom voice samples](record-custom-voice-samples.md).
      - question: |
          Do you tips for domain specific training such as gaming? 
        answer: |
          The training data should include as much domain specific terminology that you have for a specific scenario. We recommend the recording scripts include both general sentences and domain specific sentences. For example, if you plan to record 2,000 sentences, then 1,000 of them could be general sentences, and another 1,000 of them could be sentences from your target domain or the use case of your application.
      - question: |
          Is the model version the same as the engine version?
        answer: |
          No. The model version is different from the engine version. The model version means the version of the training recipe for your model and varies by the features supported and model training time. Azure Cognitive Services text-to-speech engines are updated from time to time to capture the latest language model that defines the pronunciation of the language. After you've trained your voice, you can apply your voice to the new language model by updating to the latest engine version. When a new engine is available, you're prompted to update your neural voice model. See [Update engine version for your voice model](how-to-custom-voice-create-voice.md?tabs=neural#update-engine-version-for-your-voice-model).
      - question: |
          What kind of script should be prepared for a domain specific scenario such as gaming?
        answer: |
          For general script, you can use the sample scripts per locale on [GitHub](https://github.com/Azure-Samples/Cognitive-Speech-TTS/tree/master/CustomVoice/script). For a domain specific script, you can do selection from the sentences that custom neural voice will be used to read. And you can refer to the script selection criteria [Record custom voice samples](record-custom-voice-samples.md#script-selection-criteria) to create a good corpus.
      - question: |
          Are there any successful cases of using custom neural voice in gaming?
        answer: |
          Microsoft Flight Simulator used custom neural voice service to create the voices for the air traffic controllers in several different languages using the [cross lingual](how-to-custom-voice-create-voice.md?tabs=crosslingual#train-your-custom-neural-voice-model) feature.
      - question: |
          Switching styles via SSML only works for prebuilt neural voices, right?
        answer: |
          Switching styles via SSML is only for prebuilt multi-style voices. Custom Neural Voice does support [multi-style training](how-to-custom-voice-create-voice.md?tabs=multistyle#train-your-custom-neural-voice-model) for the same model, so you can also adjust the styles via SSML. Only for the speaking styles you have created for CNV.
      - question: |
          Is it correct that after one training we can't train again unless we upload a corpus file?
        answer: |
          You can train again. There's no limit to this. However, each training will be charged the same as a new training.
      - question: |
          Can we have a more dynamic events system and characters that can respond to each other using equally dynamic voice lines, like "Whoa, that [plane] just crashed into that [tank]."?
        answer: |
          Yes. You can have dynamic content built in with static pattern. In some cases with large datasets it isn't feasible to pre-record all variations. 
      - question: |
          Can we limit the number of trainings using Azure Policy or other features? Or is there any way to avoid false training?
        answer: |
          If you want to limit the permission to training, you can limit the user roles and access. Refer to [Role-based access control for Speech resources](role-based-access-control.md).
      - question: |
          Can Microsoft add a mechanism to prevent unauthorized use or misuse of our voice when it is created?
        answer: |
          The voice model can only be used by yourselves using your own token. Microsoft also doesn't use your data. See [Data, privacy, and security](/legal/cognitive-services/speech-service/custom-neural-voice/data-privacy-security-custom-neural-voice?context=/azure/cognitive-services/speech-service/context/context).
      - question: |
          Is it necessary to place the Disclosure in the startup flow (the first screen that the user always passes)?
        answer: |
          It's not necessary to place it in the first screen. It's also fine to put it in the end role As long as they indicate it somewhere, that's fine.
      - question: |
          Are Microsoft rights and credit unnecessary?
        answer: |
          It's not necessary to give disclosure or credit for our neural text to speech voices, however it would be great for them to do so. They must give disclosure for custom neural voices. It's pretty apparent that the character they are using the voice for is not real. But when they do use custom neural voice for characters these are the design patterns.
      - question: |
          Do you have any cases about contracts or negotiation with voice actors?
        answer: |
          We have no recommendation on contract and it's up to the customer and the voice talent to negotiate the terms.
      - question: |
          Do we need to return the written permission from the voice talent back to Microsoft?
        answer: |
          Microsoft doesn't need the written permission, but you must obtain consent from your voice talent. The voice talent will also be required to record the consent statement and it must be uploaded into Speech Studio before training can begin. See [Set up voice talent for Custom Neural Voice](how-to-custom-voice-talent.md).

      
additionalContent: |

  ## Next steps
  
  - [Troubleshoot the Speech SDK](troubleshooting.md)
  - [Speech service release notes](releasenotes.md)

