---
title: Input validation for better job resiliency
description: "This article describes how to improve the resiliency of Azure Stream Analytics jobs with input schema validation"

services: stream-analytics
ms.service: stream-analytics

author: fleid
ms.author: fleide

ms.topic: how-to
ms.custom: "asaql"
ms.date: 12/10/2021
---

# Input validation in Azure Stream Analytics queries

Azure Stream Analytics (ASA) jobs process data coming from input streams. When configuring a [streaming input](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs), only the **event serialization format** has to be defined (CSV, JSON, AVRO...). With ASA, the schema of the incoming data doesn't need to be set at the input level. By schema we mean the list of fields and their data types. ASA doesn't require it because it's capable of handling **dynamic input schemas**. This means ASA expects **the list of fields (columns), and their types, to change between events (rows)**. ASA will also infer data types when none is provided explicitly, and try to implicitly cast types when needed.

**Dynamic schema handling** is a powerful feature, key to stream processing. Data streams often contain multiple event types, each with a unique schema. To route, filter, and process events on such streams, ASA has to ingest them all regardless of their schema. But that freedom comes with a downside if left unchecked: it allows malformed events to flow through the query logic. This can in turn generate errors and crashes as the query logic (and built-in functions it uses) will expect certain types in arguments.

As an example, using [ROUND](https://docs.microsoft.com/en-us/stream-analytics-query/round-azure-stream-analytics) on a `NVARCHAR(MAX)` may not end well. ASA will be able to implicitly cast it as long as the field contains a numeric value stored as a string in the input events. But when an event has that field set to `"NaN"`, then the job may fail.

**Input query validation** is the technic to use to protect the query logic from malformed or unexpected events. It adds a first step to a query, in which we make sure that the schema we submit to the core business logic matches its expectations. This article describes this technic.

## Problem statement

We will be building a new ASA job that will ingest data from a single Event Hub. As is most often the case, we are not responsible for the data producers. Here producers are IoT devices sourced from multiple hardware vendors.

After a series of meetings, we were able to define a serialization format (JSON) and schema for the events to be pushed from the devices to the Event Hub.

The schema is defined as follow:

|Field name|Field type|Field description|
|-|-|-|
|`device_Id`|Integer|Unique device identifier|
|`message_Ts`|Datetime|Message time, generated by a central gateway|
|`field_A`|String||
|`field_B`|Numeric||
|`field_C`|Array of String||

Which in turns gives us the following sample message:

```JSON
{
    "device_Id" : 1,
    "message_Ts" : "2021-12-10 10:00:00",
    "field_A" : "A String",
    "field_B" : 1.7,
    "field_C" : ["A","B"]
}
```

We can already realize a discrepancy between the schema contract and its implementation. In JSON there is no data type for datetime. It will transmitted as a string (see `message_Ts` above). ASA will be able to easily address the issue, but it shows how little trust we can have in some serialization formats to respect types. CSV is notoriously poor in that regard. This makes query validation all the more important here.

## Prerequisites

## Input query validation

## Testing input validation with unit-testing

## Next Steps