---
title: Input validation for better job resiliency
description: "This article describes how to improve the resiliency of Azure Stream Analytics jobs with input schema validation"

services: stream-analytics
ms.service: stream-analytics

author: fleid
ms.author: fleide

ms.topic: how-to
ms.custom: "asaql"
ms.date: 12/10/2021
---

# Input validation in Azure Stream Analytics queries

Azure Stream Analytics (ASA) jobs process data coming from streams. Streams are sequences of raw data that are [serialized](https://en.wikipedia.org/wiki/Serialization) to be transmitted over the network. To consume from a stream, and extract the information contained, any system needs to know about the serialization format (CSV, JSON, AVRO...) the stream uses. That's why when configuring a [streaming input](https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs) in ASA, the **event serialization format** has to be defined.

Once the data is deserialized, a schema needs to be applied to give it meaning. By schema we mean the list of fields in the stream, and their respective data types. With ASA, the schema of the incoming data doesn't need to be set at the input level. ASA instead supports **dynamic input schemas** natively. This means ASA expects **the list of fields (columns), and their types, to change between events (rows)**. ASA will also infer data types when none is provided explicitly, and try to implicitly cast types when needed.

**Dynamic schema handling** is a powerful feature, key to stream processing. Data streams often contain data from multiple sources, with multiple event types, each with a unique schema. To route, filter, and process events on such streams, ASA has to ingest them all regardless of their schema.

![Illustration of a pipeline with two fleet of devices sending data with conflicting schemas](media/input-validation/illustration.png)

But the capabilties offered by dynamic schema handling come with a potential downside. If the input data is not validated in the first steps of a query, then unexpected events can flow through the main query logic and break it. This will in turn generate errors and crashes as the built-in functions used in the logic will expect certain fields and types in arguments.

As an example, using [ROUND](https://docs.microsoft.com/en-us/stream-analytics-query/round-azure-stream-analytics) on a field of type `NVARCHAR(MAX)` may not end well (like the `readings.v` values above). ASA will be able to implicitly cast it as long as the field contains a numeric value stored as a string in the input events (`"27.8"`). But when an event has that field set to `"NaN"`, or if the field is entirely missing, then the job may fail.

**Input query validation** is the technic to use to protect the main query logic from malformed or unexpected events. It adds a first stage to a query, in which we make sure that the schema we submit to the core business logic matches its expectations. This article describes how to implement this technic.

## Problem statement

We will be building a new ASA job that will ingest data from a single event hub. As is most often the case, we are not responsible for the data producers. Here producers are IoT devices sourced from multiple hardware vendors.

After a series of meetings, we were able to define a serialization format (JSON) and a schema for the events to be pushed from the devices to the event hub.

The schema contract is defined as follow:

|Field name|Field type|Field description|
|-|-|-|
|`deviceId`|Integer|Unique device identifier|
|`readingTimestamp`|Datetime|Message time, generated by a central gateway|
|`readingStr`|String||
|`readingNum`|Numeric||
|`readingArray`|Array of String||

Which in turns gives us the following sample message:

```JSON
{
    "deviceId" : 1,
    "readingTimestamp" : "2021-12-10 10:00:00",
    "readingStr" : "A String",
    "readingNum" : 1.7,
    "readingArray" : ["A","B"]
}
```

We can already realize a discrepancy between the schema contract and its implementation. In JSON there is no data type for datetime. It will be transmitted as a string (see `readingTimestamp` above). ASA will be able to easily address the issue, but it shows the need to validate and explicitly cast types. All the more when using CSV as serialization format, since all values are then transmitted as string.

Another discrepancy exists between the incoming field types and the type system used by ASA. If ASA has [built-in types](/stream-analytics-query/data-types-azure-stream-analytics) for integer (big int), datetime, string (NVARCHAR(MAX)) and arrays, it only supports numeric via float. In certain edge cases it could cause slight drifts in precision if the input numeric was a fixed decimal and not an approximate number. If that was the case, we would also convert the numeric value to string for post processing in a system that supports fixed decimal and could detect and correct potential drifts.

This data will need to be inserted in a SQL table with the following schema:

```SQL
CREATE TABLE [dbo].[readings](
	[Device_Id] int NULL,
	[Reading_Timestamp] datetime2(7) NULL,
	[Reading_String] nvarchar(200) NULL,
	[Reading_Num] decimal(18,10) NULL,
	[Array_Count] int NULL
) ON [PRIMARY]
```

We will only count the values held in `readingArray` and insert this count in `Array_Count`.

It's a good practice to map what happens to each field as it goes through the job:

|Field|Input|Inherited type|Output requirement|Comment|
|-|-|-|-|-|
|deviceId|JSON number|bigint|integer||
|readingTimestamp|JSON string|nvarchar(MAX)|datetime2||
|readingStr|JSON string|nvarchar(MAX)|nvarchar(200)||
|readingNum|JSON number|float|decimal(18,10)||
|readingArray|JSON array(string)|array of nvarchar(MAX)|integer|to be counted|

## Prerequisites

We will develop the query in **Visual Studio Code** using the **ASA Tools** extension. The first steps of this [tutorial](https://docs.microsoft.com/en-us/azure/stream-analytics/quick-create-visual-studio-code) will guide you through installing the required components.

## First implementation

Let's start with the most basic implementation, with no input validation.

1. In VS Code we will create a new ASA project

2. We will add a new JSON file in the input folder containing the following records:

```JSON
[
    {
        "deviceId" : 1,
        "readingTimestamp" : "2021-12-10 10:00:00",
        "readingStr" : "A String",
        "readingNum" : 1.7,
        "readingArray" : ["A","B"]
    },
    {
        "deviceId" : 2,
        "readingTimestamp" : "2021-12-10 10:01:00",
        "readingStr" : "Another String",
        "readingNum" : 2.3,
        "readingArray" : ["C"]
    },
    {
        "deviceId" : 3,
        "readingTimestamp" : "2021-12-10 10:01:20",
        "readingStr" : "A Third String",
        "readingNum" : -4.8,
        "readingArray" : ["D","E","F"]
    },
    {
        "deviceId" : 1,
        "readingTimestamp" : "2021-12-10 10:02:10",
        "readingStr" : "A Forth String",
        "readingNum" : 1.2,
        "readingArray" : ["G","G"]
    }
]
```

3. Then we will [define a local input](azure/stream-analytics/visual-studio-code-local-run#define-a-local-input) referencing the JSON file we created above

## Input query validation

Define the expectations
Add the preliminary steps
Check

## Testing input validation with unit-testing

Add test cases for malformed events that could break the query but we haven't seen yet
Prep the data and write the config files
Test the query

## Next Steps
