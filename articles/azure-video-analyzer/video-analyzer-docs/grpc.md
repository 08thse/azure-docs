Quickstart: Analyze live video by using your own gRPC model

This quickstart shows you how to use Azure Video Analyzer to analyze a live video feed from a (simulated) IP camera. You'll see how to apply a computer vision model to detect objects. A subset of the frames in the live video feed is sent to an inference service. The results are sent to IoT Edge Hub.
This quickstart uses an Azure VM as an IoT Edge device, and it uses a simulated live video stream. It's based on sample code written in C#, and it builds on the Detect motion and emit events quickstart.
Prerequisites
•	An Azure account that includes an active subscription. Create an account for free if you don't already have one.
 Note
You will need an Azure subscription with at least a Contributor role. If you do not have the right permissions, please reach out to your account administrator to grant you the right permissions.
•	Visual Studio Code, with the following extensions:
o	Azure IoT Tools
 Tip
When installing Azure IoT Tools, you might be prompted to install Docker. Feel free to ignore the prompt.
o	C#
•	.NET Core 3.1 SDK.
•	If you didn't complete the Detect motion and emit events quickstart, then be sure to set up Azure resources.
 Tip
If you run into issues with Azure resources that get created, please view our troubleshooting guide to resolve some commonly encountered issues.

Review the sample video
When you set up the Azure resources, a short video of highway traffic is copied to the Linux VM in Azure that you're using as the IoT Edge device. This quickstart uses the video file to simulate a live stream.
Open an application such as VLC media player. Select Ctrl+N and then paste a link to the highway intersection sample video to start playback. You see the footage of many vehicles moving in highway traffic.
> [!VIDEO https://www.microsoft.com/en-us/videoplayer/embed/RE4LTY4]

In this quickstart, you'll use Azure Video Analyzer to detect objects such as vehicles and persons. You'll publish associated inference events to IoT Edge Hub.
Overview
 
This diagram shows how the signals flow in this quickstart. An edge module simulates an IP camera hosting a Real-Time Streaming Protocol (RTSP) server. An RTSP source node pulls the video feed from this server and sends video frames to the motion detection processor node. This processor will detect motion and upon detection will push video frames to the gRPC extension processor node.
The gRPC extension node plays the role of a proxy. It converts the video frames to the specified image type. Then it relays the image over gRPC to another edge module that runs an AI model behind a gRPC endpoint over a shared memory. In this example, that edge module is built by using the YOLOv3 model, which can detect many types of objects. The gRPC extension processor node gathers the detection results and publishes events to the IoT Hub sink node. The node then sends those events to IoT Edge Hub.
In this quickstart, you will:
1.	Create and deploy the Pipeline.
2.	Interpret the results.
3.	Clean up resources.
Create and deploy the Pipeline
Examine and edit the sample files
As part of the prerequisites, you downloaded the sample code to a folder. Follow these steps to examine and edit the sample files.
1.	In Visual Studio Code, go to src/edge. You see your .env file and a few deployment template files.
2.	The deployment.grpcyolov3icpu.template.json refers to the deployment manifest for the edge device. It includes some placeholder values. The .env file includes the values for those variables.
3.	Go to the src/cloud-to-device-console-app folder. Here you see your appsettings.json file and a few other files:
o	c2d-console-app.csproj - The project file for Visual Studio Code.
o	operations.json - A list of the operations that you want the program to run.
o	Program.cs - The sample program code. This code:
	Loads the app settings.
	Invokes direct methods that the Azure Video Analyzer module exposes. You can use the module to analyze live video streams by invoking its direct methods.
	Pauses so that you can examine the program's output in the TERMINAL window and examine the events that were generated by the module in the OUTPUT window.
	Invokes direct methods to clean up resources.
4.	Edit the operations.json file:
o	Change the link to the pipeline:
o	"topologyUrl" : "https://raw.githubusercontent.com/Azure/live-video-analytics/master/MediaGraph/topologies/motion-with-grpcExtension/2.0/topology.json"
o	Under livePipelineSet, edit the name of the graph topology to match the value in the preceding link:
o	"topologyName" : "EVROnMotionPlusGrpcExtension"
o	Under topologyDelete, edit the name:
o	"name" : "EVROnMotionPlusGrpcExtension"
 Note
Expand this and check out how the VideoAnalyzer.GrpcExtension node is implemented in the topology
Generate and deploy the IoT Edge deployment manifest
1.	Right-click the src/edge/ deployment.grpcyolov3icpu.template.json file and then select Generate IoT Edge Deployment Manifest.
 
2.	The deployment.grpcyolov3icpu.amd64.json manifest file is created in the src/edge/config folder.
3.	If you completed the Detect motion and emit events quickstart, then skip this step.
4.	Otherwise, near the AZURE IOT HUB pane in the lower-left corner, select the More actions icon and then select Set IoT Hub Connection String. You can copy the string from the appsettings.json file. Or, to ensure you've configured the proper IoT hub within Visual Studio Code, use the Select IoT hub command.
 
Note
You might be asked to provide Built-in endpoint information for the IoT Hub. To get that information, in Azure portal, navigate to your IoT Hub and look for Built-in endpoints option in the left navigation pane. Click there and look for the Event Hub-compatible endpoint under Event Hub compatible endpoint section. Copy and use the text in the box. The endpoint will look something like this:
Endpoint=sb://iothub-ns-xxx.servicebus.windows.net/;SharedAccessKeyName=iothubowner;SharedAccessKey=XXX;EntityPath=<IoT Hub name>
5.	Right-click src/edge/config/ deployment.grpcyolov3icpu.amd64.json and select Create Deployment for Single Device.
 
6.	When you're prompted to select an IoT Hub device, select avasample-iot-edge-device.
7.	After about 30 seconds, in the lower-left corner of the window, refresh Azure IoT Hub. The edge device now shows the following deployed modules:
o	The Azure Video Analyzer module, named avaedge.
o	The rtspsim module, which simulates an RTSP server and acts as the source of a live video feed.

Note
The above steps are assuming you are using the virtual machine created by the setup script. If you are using your own edge device instead, go to your edge device and run the following commands with admin rights, to pull and store the sample video file used for this quickstart:
mkdir /home/avaedgeuser/samples
mkdir /home/avaedgeuser/samples/input    
curl https://lvamedia.blob.core.windows.net/public/camera-300s.mkv > /home/avaedgeuser/samples/input/camera-300s.mkv  
chown -R lvalvaedgeuser:localusergroup /home/lvaedgeuser/samples/  
o	The avaExtension module, which is the YOLOv3 object detection model that uses gRPC as the communication method and applies computer vision to the images and returns multiple classes of object types.
 
Prepare to monitor events
1.	In Visual Studio Code, open the Extensions tab (or press Ctrl+Shift+X) and search for Azure IoT Hub.
2.	Right click and select Extension Settings.
 
3.	Search and enable “Show Verbose Message”.
 
4.	Right-click the Live Video Analytics device and select Start Monitoring Built-in Event Endpoint. You need this step to monitor the IoT Hub events in the OUTPUT window of Visual Studio Code.
 
Note
You might be asked to provide Built-in endpoint information for the IoT Hub. To get that information, in Azure portal, navigate to your IoT Hub and look for Built-in endpoints option in the left navigation pane. Click there and look for the Event Hub-compatible endpoint under Event Hub compatible endpoint section. Copy and use the text in the box. The endpoint will look something like this:
Endpoint=sb://iothub-ns-xxx.servicebus.windows.net/;SharedAccessKeyName=iothubowner;SharedAccessKey=XXX;EntityPath=<IoT Hub name>
Run the sample program
1.	To start a debugging session, select the F5 key. You see messages printed in the TERMINAL window.
2.	The operations.json code starts off with calls to the direct methods PipelineTopologyList and LivePipelineList. If you cleaned up resources after you completed previous quickstarts, then this process will return empty lists and then pause. To continue, select the Enter key.
--------------------------------------------------------------------------
Executing operation pipelineTopologyList
-----------------------  Request: pipelineTopologyList  --------------------------------------------------
{
"@apiVersion": "1.0"
}
---------------  Response: pipelineTopologyList - Status: 200  ---------------
{
"value": []
}
--------------------------------------------------------------------------
Executing operation WaitForInput
Press Enter to continue
3.	The TERMINAL window shows the next set of direct method calls:
o	A call to pipelineTopologySet that uses the preceding topologyUrl
o	A call to livePipelineSet that uses the following body:
{
  "@apiVersion": "1.0",
  "name": "Sample-Graph-1",
  "properties": {
    "topologyName": "InferencingWithGrpcExtension",
    "description": "Sample graph description",
    "parameters": [
      {
        "name": "rtspUrl",
        "value": "rtsp://rtspsim:554/media/camera-300s.mkv"
      },
      {
        "name": "rtspUserName",
        "value": "testuser"
      },
      {
        "name": "rtspPassword",
        "value": "testpassword"
      },
      {
        "name": "grpcExtensionAddress",
        "value": "tcp://avaextension:44000"
      }
    ]
  }
}
o	A call to livePipelineActivate that starts the graph instance and the flow of video.
o	A second call to livePipelineList that shows that the graph instance is in the running state.
4.	The output in the TERMINAL window pauses at a Press Enter to continue prompt. Don't select Enter yet. Scroll up to see the JSON response payloads for the direct methods you invoked.
5.	Switch to the OUTPUT window in Visual Studio Code. You see messages that the Azure Video Analyzer module is sending to the IoT hub. The following section of this quickstart discusses these messages.
6.	The pipeline continues to run and print results. The RTSP simulator keeps looping the source video. To stop the media graph, return to the TERMINAL window and select Enter.
7.	The next series of calls cleans up resources:
o	A call to livePipelineDeactivate deactivates the graph instance.
o	A call to livePipelineDelete deletes the instance.
o	A call to pipelineTopologyDelete deletes the topology.
o	A final call to pipelineTopologyList shows that the list is empty.
Interpret results
When you run the pipeline topology, the results from the gRPC extension processor node pass through the IoT Hub sink node to the IoT hub. The messages you see in the OUTPUT window contain a body section and an applicationProperties section. For more information, see Create and read IoT Hub messages.
In the following messages, the Azure Video Analyzer module defines the application properties and the content of the body.
MediaSessionEstablished event
When a media graph is instantiated, the RTSP source node attempts to connect to the RTSP server that runs on the rtspsim-live555 container. If the connection succeeds, then the following event is printed. The event type is Microsoft.VideoAnalyzer..Diagnostics.MediaSessionEstablished.

[IoTHubMonitor] [1:42:22 PM] Message received from [avasample-iot-edge-device/avaedge]:
{
  "sdp": "SDP:\nv=0\r\no=- 1617655341856633 1 IN IP4 172.18.0.6\r\ns=Matroska video+audio+(optional)subtitles, streamed by the LIVE555 Media Server\r\ni=media/camera-300s.mkv\r\nt=0 0\r\na=tool:LIVE555 Streaming Media v2020.08.19\r\na=type:broadcast\r\na=control:*\r\na=range:npt=0-300.000\r\na=x-qt-text-nam:Matroska video+audio+(optional)subtitles, streamed by the LIVE555 Media Server\r\na=x-qt-text-inf:media/camera-300s.mkv\r\nm=video 0 RTP/AVP 96\r\nc=IN IP4 0.0.0.0\r\nb=AS:500\r\na=rtpmap:96 H264/90000\r\na=fmtp:96 packetization-mode=1;profile-level-id=4D0029;sprop-parameter-sets=Z00AKeKQCgC3YC3AQEBpB4kRUA==,aO48gA==\r\na=control:track1\r\n"
},
  "applicationProperties": {
    "dataVersion": "1.0",
    "topic": "/subscriptions/{subscriptionID}/resourceGroups/{name}/providers/microsoft.media/mediaservices/hubname",
    "subject": "/livePipelines/LIVEPIPELINENAMEHERE/sources/rtspSource",
    "eventType": "Microsoft.VideoAnalyzer.Diagnostics.MediaSessionEstablished",
    "eventTime": "2020-04-09T16:42:18.1280000Z"
  }
}


In this message, notice these details:
•	The message is a diagnostics event. MediaSessionEstablished indicates that the RTSP source node (the subject) connected with the RTSP simulator and has begun to receive a (simulated) live feed.
•	In applicationProperties, subject indicates that the message was generated from the RTSP source node in the media graph.
•	In applicationProperties, eventType indicates that this event is a diagnostics event.
•	The eventTime indicates the time when the event occurred.
•	The body contains data about the diagnostics event. In this case, the data comprises the Session Description Protocol (SDP) details.
Inference event
The gRPC extension processor node receives inference results from the avaextension module. It then emits the results through the IoT Hub sink node as inference events. In these events, the type is set to entity to indicate it's an entity, such as a car or truck. The eventTime value is the UTC time when the object was detected. In the following example, three cars were detected in the same video frame, with varying levels of confidence.
[IoTHubMonitor] [1:48:04 PM] Message received from [avasample-iot-edge-device/avaedge]:
{
  "timestamp": 145589011404622,
  "inferences": [
    {
      "type": "entity",
      "entity": {
        "tag": {
          "value": "car",
          "confidence": 0.97052866
        },
        "box": {
          "l": 0.40896654,
          "t": 0.60390747,
          "w": 0.045092657,
          "h": 0.029998193
        }
      }
    },
    {
      "type": "entity",
      "entity": {
        "tag": {
          "value": "car",
          "confidence": 0.9547283
        },
        "box": {
          "l": 0.20050547,
          "t": 0.6094412,
          "w": 0.043425046,
          "h": 0.037724357
        }
      }
    },
    {
      "type": "entity",
      "entity": {
        "tag": {
          "value": "car",
          "confidence": 0.94567955
        },
        "box": {
          "l": 0.55363107,
          "t": 0.5320657,
          "w": 0.037418623,
          "h": 0.027014252
        }
      }
    },
    {
      "type": "entity",
      "entity": {
        "tag": {
          "value": "car",
          "confidence": 0.8916893
        },
        "box": {
          "l": 0.6642384,
          "t": 0.581689,
          "w": 0.034349587,
          "h": 0.027812533
        }
      }
    },
    {
      "type": "entity",
      "entity": {
        "tag": {
          "value": "car",
          "confidence": 0.8547814
        },
        "box": {
          "l": 0.584758,
          "t": 0.60079926,
          "w": 0.07082855,
          "h": 0.034121
        }
      }
    }
  ]
}
In the messages, notice the following details:
•	In applicationProperties, subject references the node in the graph topology from which the message was generated.
•	In applicationProperties, eventType indicates that this event is an analytics event.
•	The eventTime value is the time when the event occurred.
•	The body section contains data about the analytics event. In this case, the event is an inference event, so the body contains inferences data.
•	The inferences section indicates that the type is entity. This section includes additional data about the entity.
Clean up resources
If you intend to try other quickstarts, keep the resources you created. Otherwise, go to the Azure portal, go to your resource groups, select the resource group where you ran this quickstart, and delete all the resources.
Next steps
•	Try running different media graph topologies using gRPC protocol.
•	Build and run sample Azure Video Analyzer (AVA) extensions 
o	Sample YOLOv3 model
o	Sample YOLOv4 model


