---
title: Azure NetApp Files large volume performance benchmarks for Linux | Microsoft Docs
description: Describes the tested performance capabilities of a single Azure NetApp Files large volume as it pertains to Linux use cases.
services: azure-netapp-files
documentationcenter: ''
author: b-hchen
manager: ''
editor: ''

ms.assetid:
ms.service: azure-netapp-files
ms.workload: storage
ms.tgt_pltfrm: na
ms.topic: conceptual
ms.date: 05/01/2023
ms.author: anfdocs
---
# Azure NetApp Files large volume performance benchmarks for Linux

This article describes the tested performance capabilities of a single Azure NetApp Files large volume as it pertains to Linux use cases. This article explores scenarios for both scale-out and scale-up read and write workloads, involving one and many virtual machines (VMs). Knowing the performance envelope of large volumes helps you facilitate volume sizing.

Azure NetApp Files large volumes offer three service levels (*Ultra*, *Premium*, and *Standard*), each with its throughput limits. For more information, see [Service levels for Azure NetApp Files](azure-netapp-files-service-levels.md#supported-service-levels).

## Synopsis

* A single large volume can deliver sequential throughput up to the service level limits in all but the pure sequential write scenario. For sequential writes, the synthetic tests found the upper limit to be 8500 MiB/s.
* Using 8-KiB random workloads, 10,240 MiB/s isn't achievable. As such, more than 700,000 8-KiB operations were achieved.
* This article intends to show the limits of throughput and I/O.  

As I/O types shift toward metadata intensive operations, the scenario changes again. 

## Test methodologies and tools

All scenarios documented in this article used FIO, which is a synthetic workload generator designed as a storage stress test. Fundamentally, there are two models of storage performance testing:

* **Application level**   
    For application-level testing, the efforts are to drive I/O through client buffer caches in the same way that a typical application drives I/O. In general, when testing in this manner, direct I/O isn't used.
    * Except for databases (for example, Oracle, SAP HANA, MySQL (InnoDB storage engine), PostgreSQL, and Teradata), few applications use direct I/O. Instead, most applications use a large memory cache for repeated reads and a write-behind cache for asynchronous writes.
    * SPECstorage 2020 (EDA, VDA, AI, genomics, and software build), HammerDB for SQL Server, and Login VSI are typical examples of application-level testing tools. None of them uses direct I/O.

* **Storage stress test**  
    The most common parameter used in storage performance benchmarking is direct I/O. It's supported by FIO and Vdbench. DISKSPD offers support for the similar construct of memory-mapped I/O. With direct I/O, the filesystem cache is bypassed, operations for direct memory access copy are avoided, and storage tests are made fast and simple.
    * Using the direct I/O parameter makes storage testing easy. No data is read from the filesystem cache on the client. As such, the test stresses the storage protocol and service itself rather than the memory access speeds. Also, without the DMA memory copies, read and write operations are efficient from a processing perspective.
    * Take the Linux `dd` command as an example workload. Without the optional `o_direct` flag, all I/O generated by `dd` is served from the Linux buffer cache. Reads with the blocks already in memory aren't retrieved from storage. Reads resulting in a buffer-cache miss end up being read from storage using NFS read-ahead with varying results, depending on factors as mount `rsize` and client read-ahead tunables. When writes are sent through the buffer cache, they use a write-behind mechanism, which is untuned and uses a significant amount of parallelism to send the data to the storage device. You might attempt to run two independent streams of I/O, one `dd` for reads and one `dd` for writes. However, the operating system, being untuned, favors writes over reads and uses more parallelism for it.
    * Except for database, few applications use direct I/O. Instead, they take advantage of a large memory cache for repeated reads and a write-behind cache for asynchronous writes. In short, using direct I/O turns the test into a micro benchmark.

## Linux scale-out tests

This section describes performance thresholds of a single large volume on scale-out. The tests were run with the following configuration:

| Configuration | Setting |
|---|---|
| Azure VM size | E32s_v5 |
| Azure VM egress bandwidth limit | 2000 MiB/s (2 GiB/s) |
| Operating system | RHEL 8.4 |
| Large volume size | 101 TiB Ultra (10,240 MiB/s throughput) |
| Mount options | `hard,rsize=262144,wsize=262144,vers=3` |

### 256-KiB Sequential Workloads (MiB/s)

The following graph represents a 256 Kibibyte (KiB) sequential workload ranging in 10% increments from 100% read to 100% write. A 1-TiB working set was used for these tests. This graph shows that a single Azure NetApp Files large volume can handle between ~8,518 MiB/s pure sequential writes and ~9,970 MiB/s pure sequential reads.

:::image type="content" source="../media/azure-netapp-files/performance-large-volume-scale-out-sequential-workloads.png" alt-text="Graph showing 256-KiB sequential workloads of 12 VMs for one large volume." lightbox="../media/azure-netapp-files/performance-large-volume-scale-out-sequential-workloads.png":::

12 e32s_v5 VMs were used in testing. As the [Linux scale-up tests](#linux-scale-up-tests) section shows, using 12 VMs ensured that storage was the gating factor.

### 8-KiB Random Workload (IOPS)

The following graph describes the results of an 8 Kibibyte (KiB) random workload, also with a 1 TiB working set. The 8-KiB `iosize` is ubiquitous for databases and thus its selection. This graph shows that an Azure NetApp Files large volume can handle between ~474,000 pure random writes and ~709,000 pure random reads.

:::image type="content" source="../media/azure-netapp-files/performance-large-volume-scale-out-random-workloads.png" alt-text="Graph showing 8-KiB random workloads of 12 VMs for one large volume." lightbox="../media/azure-netapp-files/performance-large-volume-scale-out-random-workloads.png":::

## Linux scale-up tests 

While scale-out tests are designed to find the limits of a single large volume, scale-up tests are designed to find the upper limits of a single instance against said large volume. That is, they find out just how far a single VM can push the volume. 

Azure places network egress limits on its VMs. For network-attached storage, this setup means that the write bandwidth is capped per VM. This section demonstrates what can be done given the largest available bandwidth cap and with sufficient processors to drive said workload. 

The tests in this section were run with the following configuration:

| Configuration | Setting |
|---|---|
| Azure VM size | E104id_v5 |
| Azure VM egress bandwidth limit | 12,500 MiB/s (12.2 GiB/s) |
| Operating system | RHEL 8.4 |
| Large volume size | 101 TiB Ultra (10,240 MiB/s throughput) |
| Mount options | `hard,rsize=262144,wsize=262144,vers=3,nconnect=8` <br><br> `hard,rsize=262144,wsize=262144,vers=3` |

The graphs in this section show the results for the client-side mount option with NFSv3. For more information, see the `nconnect` section of [Linux mount options](performance-linux-mount-options.md#nconnect).

The graphs compare the advantages of `nconnect` to a non-`connected` mounted volume. In the graphs, FIO generated the workload from a single E104id-v5 instance in the East US Azure region using a 256-KiB sequential workload. Using the 256K I/O size, which is the largest I/O size recommended by Azure NetApp Files, resulted in comparable performance numbers. For more information, see the `rsize` and `wsize` section of [Linux mount options](performance-linux-mount-options.md#rsize-and-wsize).

### Linux read throughput 

The following graphs show 256-KiB sequential reads of ~10,000 MiB/s reads with `nconnect`, approximately 10 times the throughput achieved without  `nconnect`. 10,000 MiB/s is bandwidth proffered by a large volume in the Ultra service level. 6,400 MiB/s is the bandwidth for a Premium large volume. 1,600 MiB/s is the bandwidth for a Standard large volume.

:::image type="content" source="../media/azure-netapp-files/performance-large-volume-scale-up-linux-read-throughput.png" alt-text="Graphs comparing throughput for sequential read tests with and without `nconnect`." lightbox="../media/azure-netapp-files/performance-large-volume-scale-up-linux-read-throughput.png":::

### Linux write throughput

This section shows the advantages of `nconnect` for sequential write workloads. Using `nconnect` had a noticeable benefit for sequential writes, garnering the client 6,600 MiB/s, which is approximately four times greater throughput than achievable without `nconnect`. 

:::image type="content" source="../media/azure-netapp-files/performance-large-volume-scale-up-linux-write-throughput.png" alt-text="Graphs comparing throughput for sequential write test with and without `nconnect`." lightbox="../media/azure-netapp-files/performance-large-volume-scale-up-linux-write-throughput.png":::

### Linux read IOPS

Using 8-KiB random reads and `nconnect`, the E104ds_v5 instance drove ~426,000 read IOPS, which is approximately seven times the load achievable without `nconnect`. 

:::image type="content" source="../media/azure-netapp-files/performance-large-volume-scale-up-linux-read-iops.png" alt-text="Graphs comparing IOPS of random read tests with and without `nconnect`." lightbox="../media/azure-netapp-files/performance-large-volume-scale-up-linux-read-iops.png":::

### Linux write IOPS

Also using an 8-KiB I/O size, the E104ds_v5 instance drove ~405,000 write IOPS with `nconnect`. The `nconnect` mount option delivered 7.2 times greater single-instance performance than achievable without `nconnect`.

:::image type="content" source="../media/azure-netapp-files/performance-large-volume-scale-up-linux-write-iops.png" alt-text="Graphs comparing IOPS of random write tests with and without `nconnect`." lightbox="../media/azure-netapp-files/performance-large-volume-scale-up-linux-write-iops.png":::

## Next steps  

* [Requirements and considerations for large volumes](large-volumes-requirements-considerations.md)
* [Linux NFS mount options best practices for Azure NetApp Files](performance-linux-mount-options.md)
